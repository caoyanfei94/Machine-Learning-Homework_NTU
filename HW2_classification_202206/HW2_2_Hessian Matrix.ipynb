{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08729da7",
   "metadata": {},
   "source": [
    "## Homework 2-2 Hessian Matrix\n",
    "\n",
    "Author: Cao Yanfei\n",
    "\n",
    "Imagine we are training a neural network and we are trying to find out whether the model is at **a local minima like, a saddle point, or none of the above**. We can make our decision by caculating the Hessian matrix.\n",
    "\n",
    "In practice, it is really hard to find a point where the gradient equals zero or all of the eigenvalues in Hessian matrix are greater than zero. In this homework, we make the following two assumptions:\n",
    "\n",
    "1. View gradient norm less than 1e-3 as gradient equals to zero.\n",
    "2. If minimum ratio is greater than 0.5 and gradient norm is less than 1e-3, than we assume that the model is at a 'local minima like'.\n",
    "\n",
    "> Minimum ratio is defined as the proportion of positive eigenvalues.\n",
    "\n",
    "## Important Notice\n",
    "\n",
    "In this homework, students with different student IDs will get different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbb17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = '11111111'    # Fill with your student ID\n",
    "assert student_id != 'your_student_id', 'Please fill in your student_id before you start.' # assert: statify the requirement to go on ,or break down with the alarm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef8c4c",
   "metadata": {},
   "source": [
    "## Calculate Hessian Matrix\n",
    "\n",
    "The computation of Hessian is down by TA, you do not need to and should not change the following code. The only thing you need to do is to run the following blocks and determine whether the model is at a `local minima like`, `saddle point`, or `none of the above` according to the value of `gradient norm` and `minimum ratio`.\n",
    "\n",
    "### Install Package to Compute Hessian Matrix\n",
    "\n",
    "The autograd-lib library is used to compute Hessian Matrix. You can check the full document here (https://github.com/cybertronai/autograd-lib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea25fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autograd-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c001a93",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df965b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\installation\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40a9e6",
   "metadata": {},
   "source": [
    "### Define NN Model\n",
    "\n",
    "The NN model here is used to fit a single variable math function.\n",
    "$$f(x) = \\frac{sin(5 \\pi x)}{5 \\pi x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77c5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cb51a",
   "metadata": {},
   "source": [
    "### Get Pretrained Checkpoints\n",
    "\n",
    "The pretrained checkpints is done by TA. Each student will get a different checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b88938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 1ym6G7KKNkbsqSnMmnxdQKHO1JBoF0LPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c4f1b",
   "metadata": {},
   "source": [
    "### Load Pretrained Checkpoints and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2d3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the key from student_id\n",
    "import re\n",
    "\n",
    "key = student_id[-1]\n",
    "if re.match('[0-9]', key) is not None:\n",
    "    key = int(key)\n",
    "else:\n",
    "    key = ord(key) % 10\n",
    "    \n",
    "# Load checkpoint and data coresponding to the key\n",
    "model = MathRegressor()\n",
    "autograd_lib.register(model)\n",
    "\n",
    "data = torch.load('.\\\\content\\\\data.pth')[key]\n",
    "model.load_state_dict(data['model'])\n",
    "train, target = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c70f6",
   "metadata": {},
   "source": [
    "### Function to compute gradient norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d39042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute gradient norm\n",
    "def compute_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "#         print(p)\n",
    "#         Linear(in_features=1, out_features=128, bias=True)\n",
    "#         ReLU()\n",
    "#         Linear(in_features=128, out_features=1, bias=True)\n",
    "        if isinstance(p, nn.Linear):   # Learn whether object 'p' is a 'nn.Linear' type\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "    \n",
    "    grad_mean = np.mean(grads)    # Compute mean of gradient norms\n",
    "    \n",
    "    return grad_mean          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d6ad8",
   "metadata": {},
   "source": [
    "### Function to compute minimum ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c2be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code from the official document(https://github.com/cybertronai/autograd-lib)\n",
    "\n",
    "# Helper function to save activations\n",
    "def save_activations(layer, A, _):\n",
    "    '''\n",
    "    A is the input of the layer, we use batch size of 6 here\n",
    "    layer 1: A has size of (6, 1)\n",
    "    layer 2: A has size of (6, 128)\n",
    "    '''\n",
    "    activations[layer] = A\n",
    "    \n",
    "# Helper function to compute Hessian matrix\n",
    "def compute_hess(layer, _, B):\n",
    "    '''\n",
    "    B is the backprop value of the layer\n",
    "    layer 1: B has size of(6, 128)\n",
    "    layer 2: B has size of (6, 1)\n",
    "    '''\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl, ni -> nli', B, A)     # Do batch-wize outer product\n",
    "    \n",
    "    # Full Hessian\n",
    "    hess[layer] += torch.einsum('nli, nkj -> likj', BA, BA) # Do batch-wize outer product, then sum over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19edd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the minimum ratio\n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    # Compute Hessian matrix\n",
    "    # Save the gradient of each layer\n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "    \n",
    "    # Compute Hessian according to the gradient value stored in the previous step\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "    \n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "    \n",
    "    # Compute eigenvalues of the Hessian matrix\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        h_eig = torch.symeig(h).eigenvalues   # torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "    \n",
    "    ratio_mean = np.mean(minimum_ratio)    # Compute mean of minimum ratio\n",
    "    \n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308aa528",
   "metadata": {},
   "source": [
    "### Mathematical Derivation\n",
    "\n",
    "Method used here (https://en.wikipedia.org/wiki/Gaussâ€“Newton_algorithm)\n",
    "\n",
    "> Notations \\\n",
    "> $\\mathbf{A}$: the input of the layer. \\\n",
    "> $\\mathbf{B}$: the backprop value. \\\n",
    "> $\\mathbf{Z}$: the output of the layer. \\\n",
    "> $L$: the total loss, mean squared error was used here, $L=e^2$. \\\n",
    "> $w$: the weight value.\n",
    "\n",
    "Assume that the input dimension of the layer is $n$, and the output dimension of the layer is $m$.\n",
    "\n",
    "The derivative of the loss is\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left( \\frac{\\partial L}{\\partial w} \\right) _{nm} &= \\mathbf{A}_m \\mathbf{B}_n,\n",
    "\\end{align*}\n",
    "\n",
    "which can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial w} &= \\mathbf{B} \\times \\mathbf{A}.\n",
    "\\end{align*}\n",
    "\n",
    "The Hessian can be derived as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf {H}_{ij} &= \\frac{\\partial ^2 L} {\\partial w_i \\partial w_j} \\\\\n",
    "    &= \\frac {\\partial}{\\partial w_i} \\left( \\frac{\\partial L}{\\partial w_j} \\right) \\\\ \n",
    "    &= \\frac {\\partial}{\\partial w_i} \\left( \\frac{2e \\partial e}{\\partial w_j} \\right) \\\\\n",
    "    &= 2 \\frac{\\partial e}{\\partial w_i} \\frac{\\partial e}{\\partial w_j} + 2e \\frac {\\partial ^2 e}{\\partial w_j \\partial w_i}.\n",
    "\\end{align*}\n",
    "\n",
    "We neglect the second-order derivative term because the term is relatively small ($e$ is small)\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}_{ij}\n",
    "    &\\propto \\frac{\\partial e}{\\partial w_i} \\frac{\\partial e}{\\partial w_j},\n",
    "\\end{align*}\n",
    "\n",
    "and as the error $e$ is a constant\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}_{ij}\n",
    "    &\\propto \\frac{\\partial L}{\\partial w_i} \\frac{\\partial L}{\\partial w_j},\n",
    "\\end{align*}\n",
    "\n",
    "then the full Hessian becomes\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H} &\\propto (\\mathbf{B}\\times\\mathbf{A}) \\times(\\mathbf{B} \\times \\mathbf{A}).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5357142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function to compue gradient norm and minimum ratio\n",
    "def main(model, train, target):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    gradient_norm = compute_gradient_norm(model, criterion, train, target)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, train, target)\n",
    "    \n",
    "    print('Gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    \n",
    "    if gradient_norm < 1e-3:\n",
    "        if minimum_ratio > 0.5:\n",
    "            critical_point = 'a local minima like'\n",
    "        else:\n",
    "            critical_point = 'a saddle point'\n",
    "    else:\n",
    "        critical_point = 'none of a local minima like or a saddle point'\n",
    "    \n",
    "    print('The model is at {}.'.format(critical_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afda774",
   "metadata": {},
   "source": [
    "After running this block, you will get the value of `gradient norm` and `minimum ratio`. Determine whether the model is at a `local minima like`, a `saddle point`, or `none of the above`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fa8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 0.02054413128644228, minimum ratio: 0.45703125\n",
      "The model is at none of a local minima like or a saddle point.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Fix randm seed\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # Reset compute dictionaries\n",
    "    activations = defaultdict(int)    # Defaultdict(int) returns 0 when the key does not exist\n",
    "    hess = defaultdict(float)\n",
    "    \n",
    "    # compute Hessian\n",
    "    main(model, train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc596b56",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Original source: https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test] *",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
